{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import copy\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losowo generowac zbiory ze zwracaniem ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spambase = fetch_ucirepo(id=94) \n",
    "   \n",
    "X = spambase.data.features \n",
    "y = spambase.data.targets\n",
    "\n",
    "X[\"target\"] = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ionosphere = fetch_ucirepo(id=52) \n",
    "  \n",
    "X = ionosphere.data.features \n",
    "y = ionosphere.data.targets.replace({'g': 1, 'b': 0})\n",
    "\n",
    "X[\"target\"] = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is normal and what is anomaly\n",
    "X_normal = X[X[\"target\"] == 1].drop(\"target\", axis=1)\n",
    "X_anomaly = X[X[\"target\"] == 0].drop(\"target\", axis=1)\n",
    "\n",
    "print(f\"NORMAL SIZE: {len(X_normal)}\")\n",
    "print(f\"ANOMALY SIZE: {len(X_anomaly)}\")\n",
    "\n",
    "# Lekko zaszumione no_spam\n",
    "X_normal_noise = X_anomaly.sample(frac=0.16)\n",
    "X_normal_noise = pd.concat([X_normal, X_normal_noise])\n",
    "\n",
    "print(f\"NORMAL NOISED SIZE: {len(X_normal_noise)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Isolation Forest\n",
    "clf = IsolationForest(random_state=42)\n",
    "clf.fit(X_normal_noise.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_normal.values)\n",
    "y_pred = np.where(y_pred == -1, 0, 1)\n",
    "print(f\"Liczba normalnych: {np.sum(y_pred)} wśród ogółu {len(y_pred)} ANOMALIE={len(y_pred)-np.sum(y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_anomaly.values)\n",
    "y_pred = np.where(y_pred == -1, 0, 1)\n",
    "print(f\"Liczba normalnych: {np.sum(y_pred)} wśród ogółu {len(y_pred)} ANOMALIE={len(y_pred)-np.sum(y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneClassSVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = OneClassSVM(gamma='auto', kernel=\"rbf\").fit(X_normal_noise.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_normal.values)\n",
    "y_pred = np.where(y_pred == -1, 0, 1)\n",
    "print(f\"Liczba normalnych: {np.sum(y_pred)} wśród ogółu {len(y_pred)} ANOMALIE={len(y_pred)-np.sum(y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_anomaly.values)\n",
    "y_pred = np.where(y_pred == -1, 0, 1)\n",
    "print(f\"Liczba normalnych: {np.sum(y_pred)} wśród ogółu {len(y_pred)} ANOMALIE={len(y_pred)-np.sum(y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(256, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class OneClassClassifier():\n",
    "    def __init__(self, df, n_of_splits, model_type=\"nn\", batch_size=32):\n",
    "        self.n_of_splits = n_of_splits\n",
    "        self.df = df\n",
    "        self.subsets = np.array_split(df, n_of_splits)\n",
    "        self.input_dim = df.shape[1]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if model_type == \"nn\":\n",
    "            self.models = [Perceptron(self.input_dim) for _ in range(n_of_splits)]\n",
    "        elif model_type == \"rf\":\n",
    "            self.models = []\n",
    "        else:\n",
    "            raise(\"Invalid model param!\")\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def prepare_data_nn(self, idx):\n",
    "        subsets = copy.deepcopy(self.subsets)\n",
    "        for i, subset in enumerate(subsets):\n",
    "            if i == idx:\n",
    "                subset[\"target\"] = 0\n",
    "            else:\n",
    "                subset[\"target\"] = 1\n",
    "        combined_df = pd.concat(subsets, ignore_index=True)\n",
    "\n",
    "        features = torch.Tensor(combined_df.drop('target', axis=1).values)\n",
    "        targets = torch.Tensor(combined_df[\"target\"].values)\n",
    "\n",
    "        dataset = TensorDataset(features, targets)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        return dataloader\n",
    "    \n",
    "    def prepare_data_rf(self, idx):\n",
    "        subsets = copy.deepcopy(self.subsets)\n",
    "        for i, subset in enumerate(subsets):\n",
    "            if i == idx:\n",
    "                subset[\"target\"] = 0\n",
    "            else:\n",
    "                subset[\"target\"] = 1\n",
    "        combined_df = pd.concat(subsets, ignore_index=True)\n",
    "\n",
    "        # Return features and targets\n",
    "        return combined_df.drop('target', axis=1).values, torch.Tensor(combined_df[\"target\"].values)\n",
    "\n",
    "    def train_models(self, verbose=False):\n",
    "        if self.model_type == \"nn\":\n",
    "            for idx, model in enumerate(self.models):\n",
    "                model.train()\n",
    "                dataloader = self.prepare_data_nn(idx)\n",
    "                criterion = nn.BCELoss()\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "                for epoch in range(10):\n",
    "                    for inputs, labels in dataloader:\n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    if verbose:\n",
    "                        print(f'Model {idx+1}: Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
    "        if self.model_type == \"rf\":\n",
    "            for idx in range(self.n_of_splits):\n",
    "                X, y = self.prepare_data_rf(idx)\n",
    "                clf = RandomForestClassifier(max_depth=4, random_state=0)\n",
    "                clf.fit(X, y)\n",
    "                self.models.append(clf)\n",
    "                if verbose:\n",
    "                    print(f\"Model {idx+1} is ready (RandomForest)\")\n",
    "        \n",
    "    def predict_nn(self, input_sample, threshold=0.5):\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "        predictions = [model(input_sample) for model in self.models]\n",
    "\n",
    "        average_prediction = torch.mean(torch.stack(predictions), dim=0)\n",
    "        predicted_class = (average_prediction > threshold).int()\n",
    "\n",
    "        # Return avg_probability, predicted_class based on threshold\n",
    "        return average_prediction, predicted_class.item()\n",
    "\n",
    "\n",
    "one_class_classifier = OneClassClassifier(\n",
    "    n_of_splits = 3,\n",
    "    df = X_normal_noise,\n",
    "    model_type=\"rf\"\n",
    ")\n",
    "one_class_classifier.train_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN\n",
    "# 1 -> normal\n",
    "# 0 -> anomaly\n",
    "results = []\n",
    "probabilities = []\n",
    "for sample in X_normal.values:\n",
    "    input_sample=torch.Tensor(sample).reshape(1, -1)\n",
    "\n",
    "    for model in one_class_classifier.models:\n",
    "        model.eval()\n",
    "    predictions = [model(input_sample) for model in one_class_classifier.models]\n",
    "\n",
    "    average_prediction = torch.mean(torch.stack(predictions), dim=0)\n",
    "    predicted_class = (average_prediction > 0.6).int()\n",
    "    results.append(predicted_class.item())\n",
    "    probabilities.append(average_prediction.item())\n",
    "sum(results) / len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF\n",
    "sample = X_anomaly.values\n",
    "probs = [model.predict_proba(sample) for model in one_class_classifier.models]\n",
    "average_probs = np.mean(probs, axis=0)\n",
    "threshold = 0.65\n",
    "predictions = [1 if prob[1] >= threshold else 0 for prob in average_probs]\n",
    "sum(predictions) / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_probs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
